{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWY7R2O4XnM8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from collections import OrderedDict\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "oC_qmAkohC8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, fc_layers, conv_layers):\n",
        "        super().__init__()\n",
        "        self.convolution_stack = nn.Sequential(OrderedDict(conv_layers))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(OrderedDict(fc_layers))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution_stack(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "uCzs50J0ZZXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_layer(previous_size, ind, l_s_list, l_a_list):\n",
        "    name = l_a_list[ind]\n",
        "    act_name = None\n",
        "    act = None\n",
        "    current_size = None\n",
        "    if(name == 'flatten'):\n",
        "        act_name = name + str(ind+1)\n",
        "        act = nn.Flatten()\n",
        "        current_size = (previous_size[0]*previous_size[1]*previous_size[2],)\n",
        "        return current_size, act_name, act\n",
        "\n",
        "    if((name == 'conv1d') or (name == 'conv2d') or (name == 'conv3d')):\n",
        "        if(name == 'conv2d'):\n",
        "            act_name = name + str(ind+1)\n",
        "            act = nn.Conv2d(previous_size[0], l_s_list[ind]['n_channel'], l_s_list[ind]['k_size'], stride=l_s_list[ind]['s_size'], padding=l_s_list[ind]['p_size'])\n",
        "            c_size = l_s_list[ind]['n_channel']\n",
        "            height = (previous_size[1] + 2*l_s_list[ind]['p_size'] - l_s_list[ind]['k_size'])//l_s_list[ind]['s_size'] + 1\n",
        "            width = (previous_size[2] + 2*l_s_list[ind]['p_size'] - l_s_list[ind]['k_size'])//l_s_list[ind]['s_size'] + 1\n",
        "            current_size = (c_size, height, width)\n",
        "            return current_size, act_name, act\n",
        "\n",
        "    if(name == 'maxpool'):\n",
        "        act_name = name + str(ind+1)\n",
        "        act = nn.MaxPool2d(l_s_list[ind]['k_size'], stride=l_s_list[ind]['s_size'], padding=l_s_list[ind]['p_size'])\n",
        "        height = (previous_size[1] + 2*l_s_list[ind]['p_size'] - l_s_list[ind]['k_size'])//l_s_list[ind]['s_size'] + 1\n",
        "        width = (previous_size[2] + 2*l_s_list[ind]['p_size'] - l_s_list[ind]['k_size'])//l_s_list[ind]['s_size'] + 1\n",
        "        current_size = (previous_size[0], height, width)\n",
        "        return current_size, act_name, act\n",
        "\n",
        "    if(name == 'linear'):\n",
        "        act_name = name + str(ind+1)\n",
        "        act = nn.Linear(previous_size[0], l_s_list[ind])\n",
        "        current_size = (l_s_list[ind],)\n",
        "        return current_size, act_name, act\n",
        "\n",
        "    if(name == 'relu'):\n",
        "        act_name = name + str(ind+1)\n",
        "        act = nn.ReLU()\n",
        "        return previous_size, act_name, act\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_layer(previous_size, layer_size_list, layer_activation_list):\n",
        "    defined_layer = []\n",
        "    length = len(layer_size_list)\n",
        "    for i in range(0, length):\n",
        "        previous_size, act_name , act = find_layer(previous_size, i, layer_size_list, layer_activation_list)\n",
        "        val = (act_name, act)\n",
        "        defined_layer.append(val)\n",
        "\n",
        "    return previous_size, defined_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "NgoaTQPBl2Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "previous_size = train_features.shape\n",
        "previous_size = previous_size[1:]\n",
        "print(previous_size)\n",
        "\n",
        "layer_activation_list1 = ['conv2d', 'relu', 'maxpool', 'conv2d', 'relu', 'maxpool']\n",
        "layer_size_list1 = [{'n_channel': 32, 'k_size': 3, 's_size': 1, 'p_size':1},\n",
        "                    None,\n",
        "                    {'k_size': 2, 's_size': 2, 'p_size':0},\n",
        "                    {'n_channel': 64, 'k_size': 3, 's_size': 1, 'p_size':0},\n",
        "                    None,\n",
        "                    {'k_size': 2, 's_size': 2, 'p_size':0}]\n",
        "\n",
        "previous_size1, convolution_layer = model_layer(previous_size, layer_size_list1, layer_activation_list1)\n",
        "print(previous_size1)\n",
        "print(convolution_layer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ8KivhbYBbc",
        "outputId": "70b13a15-76cb-4cf7-de9d-342279801163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "(64, 6, 6)\n",
            "[('conv2d1', Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('relu2', ReLU()), ('maxpool3', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)), ('conv2d4', Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))), ('relu5', ReLU()), ('maxpool6', MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = 1\n",
        "for ele in previous_size1:\n",
        "    val = val*ele\n",
        "\n",
        "print(val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nznBR5jUdTMI",
        "outputId": "70ef1c00-5bb1-4786-9aad-33159af29016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "previous_size2 = (val,)\n",
        "layer_activation_list = ['linear', 'linear', 'linear']\n",
        "layer_size_list = [600, 120, 10]\n",
        "previous_size3, fully_connected_layer = model_layer(previous_size2, layer_size_list, layer_activation_list)\n",
        "print(fully_connected_layer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjflTZvA_QWf",
        "outputId": "8b8e2553-c106-428e-fe22-e602e24fde0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('linear1', Linear(in_features=2304, out_features=600, bias=True)), ('linear2', Linear(in_features=600, out_features=120, bias=True)), ('linear3', Linear(in_features=120, out_features=10, bias=True))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer = [\n",
        "#             ('linear1', nn.Linear(28*28, 512)),\n",
        "#             ('relu1', nn.ReLU()),\n",
        "#             ('linear2', nn.Linear(512, 512)),\n",
        "#             ('relu2', nn.ReLU()),\n",
        "#             ('linear3', nn.Linear(512, 10))\n",
        "#             ]\n",
        "\n",
        "model = NeuralNetwork(fully_connected_layer, convolution_layer)\n"
      ],
      "metadata": {
        "id": "lNWX9gz0ktx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n"
      ],
      "metadata": {
        "id": "hDewNj5oYUH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5kI5NGkcYu8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orN29VwoY18r",
        "outputId": "c09d0c09-5ac8-42c2-e9f4-f416558a2ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.307269  [   64/60000]\n",
            "loss: 0.584017  [ 6464/60000]\n",
            "loss: 0.402192  [12864/60000]\n",
            "loss: 0.697057  [19264/60000]\n",
            "loss: 0.543448  [25664/60000]\n",
            "loss: 0.392037  [32064/60000]\n",
            "loss: 0.410415  [38464/60000]\n",
            "loss: 0.300309  [44864/60000]\n",
            "loss: 0.394690  [51264/60000]\n",
            "loss: 0.293399  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.365496 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.295698  [   64/60000]\n",
            "loss: 0.402438  [ 6464/60000]\n",
            "loss: 0.115554  [12864/60000]\n",
            "loss: 0.294770  [19264/60000]\n",
            "loss: 0.361781  [25664/60000]\n",
            "loss: 0.287294  [32064/60000]\n",
            "loss: 0.158949  [38464/60000]\n",
            "loss: 0.113525  [44864/60000]\n",
            "loss: 0.438683  [51264/60000]\n",
            "loss: 0.201499  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.323657 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.285046  [   64/60000]\n",
            "loss: 0.286256  [ 6464/60000]\n",
            "loss: 0.284902  [12864/60000]\n",
            "loss: 0.256451  [19264/60000]\n",
            "loss: 0.272612  [25664/60000]\n",
            "loss: 0.185140  [32064/60000]\n",
            "loss: 0.240520  [38464/60000]\n",
            "loss: 0.173115  [44864/60000]\n",
            "loss: 0.199645  [51264/60000]\n",
            "loss: 0.210699  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.280900 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.130831  [   64/60000]\n",
            "loss: 0.118128  [ 6464/60000]\n",
            "loss: 0.114065  [12864/60000]\n",
            "loss: 0.114581  [19264/60000]\n",
            "loss: 0.181670  [25664/60000]\n",
            "loss: 0.273392  [32064/60000]\n",
            "loss: 0.230968  [38464/60000]\n",
            "loss: 0.245146  [44864/60000]\n",
            "loss: 0.367991  [51264/60000]\n",
            "loss: 0.239174  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.289340 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.216004  [   64/60000]\n",
            "loss: 0.290106  [ 6464/60000]\n",
            "loss: 0.186061  [12864/60000]\n",
            "loss: 0.211742  [19264/60000]\n",
            "loss: 0.316613  [25664/60000]\n",
            "loss: 0.136458  [32064/60000]\n",
            "loss: 0.277442  [38464/60000]\n",
            "loss: 0.170702  [44864/60000]\n",
            "loss: 0.236553  [51264/60000]\n",
            "loss: 0.173720  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.262527 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMcJ7-x3Y_R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HT_n3EEJjI-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSePewGejI0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_odMLOfYjIqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "y02Llf9fjIbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sJ8rEKilYbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}